var relearn_searchindex = [
  {
    "breadcrumb": "Academic anecdotes \u003e Old courses",
    "content": "Game Theory in Economics and Evolution (Fall 2016) Exploration of human and animal interactions: integrating evolutionary and economic perspectives to investigate individual and social behaviour.\nIntroduction We’re taking our first steps towards flipping the classroom! Here you can find video lectures and notes that replace in-class lectures.\n01 The Ultimatum Game 02 Extensive and normal form games 03 Dominance and Pareto optimality 04 Symmetric and zero-sum games 05 Sotto vs. Blotto and mixed Nash equilibria 06 Deriving mixed Nash equilibria 07 Deriving Expected Utility Theory 08 Evolutionary Game Theory 09 An asymmetric evolutionary game 10 Multiplayer games 11 Public goods with punishment 12 Repeated games Lecture notes Here’s the whole collection of lecture notes for the videos:\n01 The Ultimatum Game.pdf 02 Extensive and normal form games.pdf 03 Dominance and Pareto Optimality.pdf 04 Symmetric and zero-sum games.pdf 05 Sotto vs. Blotto and mixed Nash equilibria.pdf 06 Deriving mixed Nash equilibria.pdf 07 Deriving Expected Utility Theory.pdf 08 Evolutionary Game Theory.pdf 09 An asymmetric evolutionary game.pdf 10 Multiplayer games.pdf 11 Public goods with punishment.pdf 12 Repeated games.pdf",
    "description": "Game Theory in Economics and Evolution (Fall 2016))\nExploration of human and animal interactions: integrating evolutionary and economic perspectives to investigate individual and social behaviour.",
    "tags": [],
    "title": "UBC ISCI 344 (2016)",
    "uri": "/~rikblok/teaching/past/isci344/index.html"
  },
  {
    "breadcrumb": "Mathematical musings",
    "content": "Remember Euler’s number, $e=2.71828$… ? One of the Bernoulli boys showed that it’s the limit of $(1 + 1/n)^n$ as $n$ goes to infinity. But if $n$ goes to infinity then we should be able to add an arbitrary constant $c$ to the denominator without changing the result. So, more generally,\n$$e = \\lim_{n\\rightarrow \\infty} \\left(1+\\frac{1}{n+c}\\right)^n.$$The question that came to my mind then is, what is the “best” constant to choose? It turns out you can show it’s $c=-1/2$. In other words, the limit of $(1+1/(n-1/2))^n$ converges to $e$ faster than Bernoulli’s formula (or any other $c$). In fact, it’s 99% accurate for $n=3$ (versus $n=50$ for Bernoulli).\nDerivation Here’s how I figured it out. Let’s call the $n$-th number in the sequence $E_n$:\n$$E_n = \\left(1+\\frac{1}{n+c}\\right)^n.$$Ideally, we want $E_n=e$ for all $n$. But then $c$ is no longer a constant. In fact, we can isolate $c$ in the above equation (with $E_n=e$) to find out how $c$ would depend on $n$:\n$$c(n) = \\left( e^{1/n} - 1 \\right)^{-1} - n.$$Now we want to know if $c$ converges to a constant as $n\\rightarrow\\infty$. But that’s tricky. It becomes much simpler if we take $u=1/n$ and look at what happens as $u\\rightarrow 0$.\n$$c(u) = \\left( e^u - 1 \\right)^{-1} - \\frac{1}{u}.$$Then we can expand $c$ as a Taylor series around $u=0$ (effectively, a Taylor expansion around $n=\\infty$, which is pretty cool!) to get\n$$c(u) \\approx -\\frac{1}{2} + \\frac{u}{12} + \\cdots$$So the best choice as a constant for large $n$ is $c=-1/2$ which gives a sequence\n$$E_n^{(1)} = \\left(1+\\frac{1}{n - 1/2}\\right)^n = \\left(\\frac{2 n + 1}{2 n - 1}\\right)^n.$$Higher order terms Including higher order terms in the approximation allows to find sequences that converge even faster! For example, the next order approximation would be $c(n) = -1/2 + 1/(12 n)$, which would give a sequence\n$$E_n^{(2)} = \\left( \\frac{12 n^2 + 6 n + 1}{12 n^2 - 6 n + 1} \\right)^n.$$It’s not as pretty an expression but it converges very quickly! It’s already more than 99.8% accurate for $n=1$! (For $n=1$ the result simplifies to the fraction $E_1^{(2)}=19/7\\approx 2.714$.)\nSummary I found replacing $c=0$ in the sequence $\\left(1+\\frac{1}{n+c}\\right)^n$ with $c=-1/2$ makes it converge to Euler’s number much faster as $n\\rightarrow \\infty$. Does it matter? Probably not. But I sure had a fun afternoon! :-)\n— Rik Blok, 2014",
    "description": "I found a sequence that converges to Euler’s constant faster than Bernoulli’s formula.",
    "tags": [],
    "title": "Euler's constant: An improved sequence (2014)",
    "uri": "/~rikblok/math/eulers_constant/index.html"
  },
  {
    "breadcrumb": "Academic anecdotes \u003e Old courses",
    "content": "",
    "description": "Models in Science (Fall 2009)\nMeaning, nature, use, strengths and limitations of models as investigative tools in all scientific disciplines. Detailed investigation of selected model systems from different scientific disciplines.",
    "tags": [],
    "title": "UBC ISCI 422 (2009)",
    "uri": "/~rikblok/teaching/past/isci422/index.html"
  },
  {
    "breadcrumb": "Research ramblings \u003e Published",
    "content": "Multimodal pattern formation in phenotype distributions of sexual populations Doebeli, \rBlok, \rLeimar \u0026 Dieckmann\r(2007)\rDoebeli, \rM., \rBlok, \rH., \rLeimar, \rO. \u0026 Dieckmann, \rU.\r \r(2007).\r Multimodal pattern formation in phenotype distributions of sexual populations.\rProceedings of the Royal Society B: Biological Sciences, 274(1608). 347–357.\rhttps://doi.org/10.1098/rspb.2006.3725\rAbstract During bouts of evolutionary diversification, such as adaptive radiations, the emerging species cluster around different locations in phenotype space. How such multimodal patterns in phenotype space can emerge from a single ancestral species is a fundamental question in biology. Frequency-dependent competition is one potential mechanism for such pattern formation, as has previously been shown in models based on the theory of adaptive dynamics. Here, we demonstrate that also in models similar to those used in quantitative genetics, phenotype distributions can split into multiple modes under the force of frequency-dependent competition. In sexual populations, this requires assortative mating, and we show that the multimodal splitting of initially unimodal distributions occurs over a range of assortment parameters. In addition, assortative mating can be favoured evolutionarily even if it incurs costs, because it provides a means of alleviating the effects of frequency dependence. Our results reveal that models at both ends of the spectrum between essentially monomorphic (adaptive dynamics) and fully polymorphic (quantitative genetics) yield similar results. This underscores that frequency-dependent selection is a strong agent of pattern formation in phenotype distributions, potentially resulting in adaptive speciation.\nDownload",
    "description": "Multimodal pattern formation in phenotype distributions of sexual populations",
    "tags": [],
    "title": "Doebeli, Blok, Leimar \u0026 Dieckmann (2007)",
    "uri": "/~rikblok/research/published/doebeli07/index.html"
  },
  {
    "breadcrumb": "Research ramblings \u003e Published",
    "content": "A tale of two cycles - distinguishing quasi-cycles and limit cycles in finite predator-prey populations Pineda-Krch, \rBlok, \rDieckmann \u0026 Doebeli\r(2007)\rPineda-Krch, \rM., \rBlok, \rH., \rDieckmann, \rU. \u0026 Doebeli, \rM.\r \r(2007).\r A tale of two cycles - distinguishing quasi-cycles and limit cycles in finite predator-prey populations.\rOikos, 116(1). 53–64.\rhttps://doi.org/10.1111/j.2006.0030-1299.14940.x\rAbstract Periodic predator-prey dynamics in constant environments are usually taken as indicative of deterministic limit cycles. It is known, however, that demographic stochasticity in finite populations can also give rise to regular population cycles, even when the corresponding deterministic models predict a stable equilibrium. Specifically, such quasi-cycles are expected in stochastic versions of deterministic models exhibiting equilibrium dynamics with weakly damped oscillations. The existence of quasi-cycles substantially expands the scope for natural patterns of periodic population oscillations caused by ecological interactions, thereby complicating the conclusive interpretation of such patterns. Here we show how to distinguish between quasi-cycles and noisy limit cycles based on observing changing population sizes in predator-prey populations. We start by confirming that both types of cycle can occur in the individual-based version of a widely used class of deterministic predator-prey model. We then show that it is feasible and straightforward to accurately distinguish between the two types of cycle through the combined analysis of autocorrelations and marginal distributions of population sizes. Finally, by confronting these results with real ecological time series, we demonstrate that by using our methods even short and imperfect time series allow quasi-cycles and limit cycles to be distinguished reliably.\nDownload",
    "description": "A tale of two cycles - distinguishing quasi-cycles and limit cycles in finite predator-prey populations",
    "tags": [],
    "title": "Pineda-Krch, Blok, Dieckmann \u0026 Doebeli (2007)",
    "uri": "/~rikblok/research/published/pinedakrch07/index.html"
  },
  {
    "breadcrumb": "Research ramblings \u003e Published",
    "content": "Scale-free extinction dynamics in spatially structured host–parasitoid systems Killingback, \rBlok \u0026 Doebeli\r(2006)\rKillingback, \rT., \rBlok, \rH. \u0026 Doebeli, \rM.\r \r(2006).\r Scale-free extinction dynamics in spatially structured host–parasitoid systems.\rJournal of Theoretical Biology, 241(4). 745–750.\rhttps://doi.org/10.1016/j.jtbi.2006.01.010\rAbstract Much of the work on extinction events has focused on external perturbations of ecosystems, such as climatic change, or anthropogenic factors. Extinction, however, can also be driven by endogenous factors, such as the ecological interactions between species in an ecosystem. Here we show that endogenously driven extinction events can have a scale-free distribution in simple spatially structured host-parasitoid systems. Due to the properties of this distribution there may be many such simple ecosystems that, although not strictly permanent, persist for arbitrarily long periods of time. We identify a critical phase transition in the parameter space of the host-parasitoid systems, and explain how this is related to the scale-free nature of the extinction process. Based on these results, we conjecture that scale-free extinction processes and critical phase transitions of the type we have found may be a characteristic feature of many spatially structured, multi-species ecosystems in nature. The necessary ingredient appears to be competition between species where the locally inferior type disperses faster in space. If this condition is satisfied then the eventual outcome depends subtly on the strength of local superiority of one species versus the dispersal rate of the other.\nDownload",
    "description": "Scale-free extinction dynamics in spatially structured host–parasitoid systems",
    "tags": [],
    "title": "Killingback, Blok \u0026 Doebeli (2006)",
    "uri": "/~rikblok/research/published/killingback06/index.html"
  },
  {
    "breadcrumb": "Academic anecdotes \u003e Old courses",
    "content": "",
    "description": "Electricity, Light and Radiation (Summer 2003)\nIntroduction to optics, electricity and magnetism, electric circuits, radioactivity, including biological applications.",
    "tags": [],
    "title": "UBC PHYS 102 (2003)",
    "uri": "/~rikblok/teaching/past/phys102/index.html"
  },
  {
    "breadcrumb": "Research ramblings \u003e Presented",
    "content": "Self-affine timeseries analysis. Guest lecture for U.B.C. Physics 510: Stochastic Processes in Physics Blok\r(2003)\rBlok, \rH.\r \r(2003).\r \rSelf-affine timeseries analysis.\rAbstract A brief introduction to Lévy flight and fractional Brownian motion from the experimentalist’s perspective. Simple tools to analyze these timeseries, the Zipf plot and dispersional analysis, are presented. As a demonstration, these tools are applied to financial and meteorological data to determine the Lévy and Hurst exponents.\nDownload",
    "description": "Self-affine timeseries analysis. Guest lecture for U.B.C. Physics 510: Stochastic Processes in Physics",
    "tags": [],
    "title": "Blok (2003)",
    "uri": "/~rikblok/research/presented/blok03/index.html"
  },
  {
    "breadcrumb": "Research ramblings \u003e Presented",
    "content": "Rock, paper and scissors in space: A demonstration of R2DToo. Blok\r(2002)\rBlok, \rH.\r \r(2002).\r \rRock, paper and scissors in space: A demonstration of R2DToo.\rAbstract Presentation given at the Dec. 2, 2002 SOWD Lab Meeting. A demonstration of how the simulation tool R2DToo can be used to solve real problems.\nDownload",
    "description": "Rock, paper and scissors in space: A demonstration of R2DToo.",
    "tags": [],
    "title": "Blok (2002)",
    "uri": "/~rikblok/research/presented/blok02b/index.html"
  },
  {
    "breadcrumb": "Research ramblings \u003e Presented",
    "content": "Statistical properties of financial timeseries Blok\r(2002)\rBlok, \rH.\r \r(2002).\r \rStatistical properties of financial timeseries.\rAbstract A brief introduction to Lévy flight and fractional Brownian motion from the experimentalist’s perspective. Simple tools to analyze these timeseries, the Zipf plot and dispersional analysis, are presented. As a demonstration, these tools are applied to intraday foreign exchange data to determine the Lévy and Hurst exponents.\nDownload",
    "description": "Statistical properties of financial timeseries",
    "tags": [],
    "title": "Blok (2002)",
    "uri": "/~rikblok/research/presented/blok02/index.html"
  },
  {
    "breadcrumb": "Mathematical musings",
    "content": "My wife and I live in a twenty eight unit condominium which shares the cost of natural gas. Each unit has its own gas fireplace which we find is sufficient to heat our unit all year long, without resorting to electric baseboard heaters. This makes for an interesting problem in game theory: at what price level can we expect the residents to switch from heating with gas to heating with electricity?\nThe problem is only interesting when the cost of heating with gas is on the same order as electric heat. If one source is much cheaper than the other then it is the rational choice. As I write this the cost of gas is roughly (if I did the math right!) 3/4 the cost of electricity (measured in dollars per unit of energy). This neglects some issues such as efficiency of turning the energy into heat, etc. but at least it gives us a ballpark figure. Clearly, the problem is relevant.\nI’m going to demonstrate two solutions to the problem. The first ignores game theory and gives a trivial, intuitive result. The second, using game theory, gives a more likely–and drastically worse–result.\nSolution 1: Self-consistency First, some definitions:\n$T$ total heating cost over some fixed period (eg. one year), $N$ number of units in condo (eg. $N=28$), $E$ total units of energy used to heat home, $f$ ratio of gas price rate to electricity rate (eg. $f=3/4$), $r_e$ cost per unit energy for electricity, $r_g$ cost per unit energy for gas ($r_g=f r_e$), and $g$ fraction of heat generated by gas, for a single resident (between zero and one). Ok, to state it mathematically, we want to find the fraction $g$ which minimizes the total cost $T$ each resident spends. For this first solution, we assume each resident is going to do the same thing because they all want to minimize $T$.\nSo each resident’s cost for electric heat is $(1-g) E r_e$ and the total cost of gas for the entire building is $N g E r_g$, which is split uniformly between the $N$ condos. So the total cost to each condo is\n$$ T = (1-g) E r_e + g E r_g = E r_e [1+(f-1)g]. $$So what would each resident choose for $g$ in order to minimize $T$? Simple: if $f\u003c1$ then choose $g=1$ and if $f\u003e1$ then choose $g=0$. This just means the rational way to heat your home is with whichever source is cheaper. That seems obvious doesn’t it?\nSolution 2: Game theoretic If you agree with the first solution this one might surprise you. Again, we need a few definitions. Instead of everybody applying the same behaviour $g$, lets consider what I should do as a resident versus what everybody else is doing:\n$g_\\text{me}$ the fraction $g$ for me, as a resident, or $g_\\text{other}$ the fraction $g$ averaged over all other residents. Now there are three costs: my electricity, $(1-g_\\text{me}) E r_e$, my gas, $g_\\text{me} E r_g$, and everybody else’s gas, $(N-1) g_\\text{other} E r_g$. As before, these last two terms are shared by all $N$ residents so my total cost is\n$$ \\begin{array}{rl} T \u0026 = (1-g_\\text{me}) E r_e + [g_\\text{me} E r_g + (N-1) g_\\text{other} E r_g]/N \\\\ \u0026 = E r_e [1+(N-1) g_\\text{other} f / N + (f/N - 1) g_\\text{me}]. \\end{array} $$Notice that I only have control over my own actions, $g_\\text{me}$. I can’t hope to influence other people’s behaviour so $g_\\text{other}$ is effectively constant, independent of what I do. So, to minimize T all I can do is try to minimize the very last term $(f/N - 1) g_\\text{me}$. This is achieved with $g_\\text{me}=1$ when $f\u003cN$ and $g_\\text{me}=0$ when $f\u003eN$.\nTragedy of the commons Compare these two solutions: the first says I should use gas only if it is cheaper than electricity, but the second says I should keep using it until it is $N$ times more expensive than electricity! ($N=28$ in my building.)\nIf that’s the optimal behaviour for me, then the same should hold for every resident in the building. So, when $1\u003cf\u003cN$ we are all going to be paying more for heating than we need to! Strange but true. This dilemma is known as the tragedy of the commons. It happens because nobody can improve their situation by changing their behaviour unless everyone else changes, too.\nFortunately, there are ways to get around this kind of dilemma. What you have to do is change the rules of the game. For example, if the price of gas got too high we could have an emergency strata meeting to vote on the option of shutting off the gas to the entire building. (Other, less totalitarian solutions are probably also available…)\n— Rik Blok, 2002",
    "description": "My wife and I live in a twenty eight unit condominium which shares the cost of natural gas.  At what price level can we expect the residents to switch from heating with gas to heating with electricity?",
    "tags": [],
    "title": "Shared Gas: A 'Tragedy' of the Commons (2002)",
    "uri": "/~rikblok/math/shared_gas/index.html"
  },
  {
    "breadcrumb": "Research ramblings \u003e Presented",
    "content": "Can memes drive genes? Blok\r(2001)\rBlok, \rH.\r \r(2001).\r \rCan memes drive genes?.\rAbstract Assuming culture is transmitted horizontally (via imitation) a model was constructed to determine the conditions under which culture can dominate genetic evolution (“get off the leash” according to Blackmore (\rCitation: 2000 Blackmore, \rS. \r(2000).\r \rThe meme machine.\r \rOxford University Press.\r)\r). Two requirements were found: (1) culture must compete with genes (required only for the effect to be empirically testable); and (2) Interactions between individuals must be confined to small groups or neighbourhoods. The model was tested via analysis and simulation.\nIn this talk I will present the model, analysis, and simulation results. Feedback is appreciated.\nDownload",
    "description": "Can memes drive genes?",
    "tags": [],
    "title": "Blok (2001)",
    "uri": "/~rikblok/research/presented/blok01/index.html"
  },
  {
    "breadcrumb": "Research ramblings \u003e Presented",
    "content": "On the nature of the stock market: Simulations and experiments. Final PhD oral defense Blok\r(2000)\rBlok, \rH.\r \r(2000).\r \rOn the nature of the stock market: Simulations and experiments (Departmental defense).\rDownload",
    "description": "On the nature of the stock market: Simulations and experiments. Final PhD oral defense",
    "tags": [],
    "title": "Blok (2000)",
    "uri": "/~rikblok/research/presented/blok00c/index.html"
  },
  {
    "breadcrumb": "Research ramblings \u003e Published",
    "content": "On the nature of the stock market: Simulations and experiments Blok\r(2000)\rBlok, \rH.\r \r(2000).\r \rOn the nature of the stock market: Simulations and experiments\r. \rUniversity of British Columbia\r Retrieved from \rhttp://hdl.handle.net/2429/11108\rAbstract Over the last few years there has been a surge of activity within the physics community in the emerging field of Econophysics - the study of economic systems from a physicist’s perspective. Physicists tend to take a different view than economists and other social scientists, being interested in such topics as phase transitions and fluctuations.\nIn this dissertation two simple models of stock exchange are developed and simulated numerically. The first is characterized by centralized trading with a market maker. Fluctuations are driven by a stochastic component in the agents’ forecasts. As the scale of the fluctuations is varied a critical phase transition is discovered. Unfortunately, this model is unable to generate realistic market dynamics.\nThe second model discards the requirement of centralized trading. In this case the stochastic driving force is Gaussian-distributed “news events” which are public knowledge. Under variation of the control parameter the model exhibits two phase transitions: both a first- and a second-order (critical).\nThe decentralized model is able to capture many of the interesting properties observed in empirical markets such as fat tails in the distribution of returns, a brief memory in the return series, and long-range correlations in volatility. Significantly, these properties only emerge when the parameters are tuned such that the model spans the critical point. This suggests that real markets may operate at or near a critical point, but is unable to explain why this should be. This remains an interesting open question worth further investigation.\nOne of the main points of the thesis is that these empirical phenomena are not present in the stochastic driving force, but emerge endogenously from interactions between agents. Further, they emerge despite the simplicity of the modeled agents; suggesting complex market dynamics do not arise from the complexity of individual investors but simply from interactions between (even simple) investors.\nAlthough the emphasis of this thesis is on the extent to which multi-agent models can produce complex dynamics, some attempt is also made to relate this work with empirical data. Firstly, the trading strategy applied by the agents in the second model is demonstrated to be adequate, if not optimal, and to have some surprising consequences.\nSecondly, the claim put forth by Sornette et al. (\rCitation: 1996 Sornette, \rD., \rJohansen, \rA. \u0026 Bouchaud, \rJ.\r \r(1996).\r Stock Market Crashes, Precursors and Replicas.\rJournal de Physique I, 6(1). 9.\rhttps://doi.org/10.1051/jp1:1996135\r)\rthat large financial crashes may be heralded by accelerating precursory oscillations is also tested. It is shown that there is weak evidence for the existence of log-periodic precursors but the signal is probably too indistinct to allow for reliable predictions.\nDownload Individual sections\nFront matter Chapter 1: Introduction Chapter 2: Centralized Stock Exchange Model Chapter 3: Decentralized Stock Exchange Model Chapter 4: Analysis and results: Phase space Chapter 5: Analysis and results: Empirical results Chapter 6: Experiments with a hypothetical portfolio Chapter 7: Concluding remarks Bibliography Appendix A: Discounted least-squares curve fitting Appendix B: Sampling discrete processes Appendix C: Long-range memory: The Hurst exponent",
    "description": "On the nature of the stock market: Simulations and experiments",
    "tags": [],
    "title": "Blok (2000)",
    "uri": "/~rikblok/research/published/blok00b/index.html"
  },
  {
    "breadcrumb": "Academic anecdotes \u003e Old courses",
    "content": "",
    "description": "Elements of Physics (Winter 2000)\nThermometry, thermal properties of matter, heat, oscillations, waves, sound, wave optics; geometrical optics, elementary electricity and magnetism, simple DC and AC circuits.",
    "tags": [],
    "title": "UBC PHYS 153 (2000)",
    "uri": "/~rikblok/teaching/past/phys153/index.html"
  },
  {
    "breadcrumb": "Research ramblings \u003e Presented",
    "content": "On the nature of the stock market: Simulations and experiments. Departmental PhD oral defense Blok\r(2000)\rBlok, \rH.\r \r(2000).\r \rOn the nature of the stock market: Simulations and experiments (Final defense).\rDownload",
    "description": "On the nature of the stock market: Simulations and experiments. Departmental PhD oral defense",
    "tags": [],
    "title": "Blok (2000)",
    "uri": "/~rikblok/research/presented/blok00/index.html"
  },
  {
    "breadcrumb": "Research ramblings \u003e Published",
    "content": "Synchronous versus asynchronous updating in the “game of Life” Blok \u0026 Bergersen\r(1999)\rBlok, \rH. \u0026 Bergersen, \rB.\r \r(1999).\r Synchronous versus asynchronous updating in the “game of Life”.\rPhysical Review E, 59(4). 3876–3879.\rhttps://doi.org/10.1103/PhysRevE.59.3876\rAbstract The rules for the “game of Life” are modified to allow for only a random fraction of sites to be updated in each time step. Under variation of this fraction from the parallel updating limit down to the Poisson limit, a critical phase transition is observed that explains why the game of Life appears to obey self-organized criticality. The critical exponents are calculated and the static exponents appear to belong to the directed percolation universality class in 2+1 dimensions. The dynamic exponents, however, are nonuniversal, as seen in other systems with multiple absorbing states.\nDownload",
    "description": "Synchronous versus asynchronous updating in the “game of Life”",
    "tags": [],
    "title": "Blok \u0026 Bergersen (1999)",
    "uri": "/~rikblok/research/published/blok99/index.html"
  },
  {
    "breadcrumb": "Research ramblings \u003e Presented",
    "content": "Modelling intentionality: The gambler. Presentation for UBC Phys 510 Blok\r(1998)\rBlok, \rH.\r \r(1998).\r \rModelling Intentionality: The Gambler.\rDownload",
    "description": "Modelling intentionality: The gambler. Presentation for UBC Phys 510",
    "tags": [],
    "title": "Blok (1998)",
    "uri": "/~rikblok/research/presented/blok98/index.html"
  },
  {
    "breadcrumb": "Research ramblings \u003e Presented",
    "content": "Extra! Extra! Critical update on ‘Life’. Presentation for Peter Wall Inst. Adv. Science, Crisis Points Group, UBC Blok\r(1998)\rBlok, \rH.\r \r(1998).\r \rExtra! Extra! Critical Update on Life.\rDownload",
    "description": "Extra! Extra! Critical update on ‘Life’. Presentation for Peter Wall Inst. Adv. Science, Crisis Points Group, UBC",
    "tags": [],
    "title": "Blok (1998)",
    "uri": "/~rikblok/research/presented/blok98b/index.html"
  },
  {
    "breadcrumb": "Research ramblings \u003e Unpublished notes",
    "content": "Discounted Least Squares Curve Fitting Blok\r(1997)\rBlok, \rH.(1997, 7/20). Retrieved from \rhttp://www.zoology.ubc.ca/~rikblok/wiki/doku.php?id=random_research:rik_s_notes:discounted_least_squares:start\rVersion 1 — Rik Blok, 1997-07-20\n1. Introduction I recently wrote some code to make simple forecasts in a time series (a steadily accumulating set of \\((x,y)\\) data points). For its simplicity, I chose a least-squares fit to a straight line. The underlying behaviour of the system was continuously changing so it was unreasonable to expect the same parameters to be valid for all the data. As new data came in, I expected old data to become irrelevant, and I handled this by only fitting over the last \\(N\\) data points. Unfortunately, it became evident that this arbitrary parameter \\(N\\) I had chosen was very important to the fit, often producing pathological results: as \\(N\\) new data points were accumulated after an outlier (a strongly atypical \\(y\\)-value), it would suddenly be dropped from consideration and the forecast would undergo a discontinuous “jump”. I began to wonder if there was a way of steadily discounting the relevance of past data in a smoother and more natural way…\n(Note: much of the text written here is a blatant copy from Press et al. (\rCitation: 1992 Press, \rW., \rTeukolsky, \rS., \rVetterling, \rW. \u0026 Flannery, \rB. \r(1992).\r \rNumerical Recipes in C: The Art of Scientific Computing (Second).\r \rCambridge University Press. Retrieved from \rhttp://www.nr.com/\r)\r. They said it so well, I could not word it any better myself. In Chapter 16 they derive the least-squares technique I described above.)\n2. Solution We use the index \\(i\\) to label our data points where \\(i=0\\) indicates the most recently acquired data and \\(i=1,2,3,\\ldots\\) indicate successively older data. Each data point consists of a triplet \\((x,y,\\sigma)\\) where \\(x\\) is the independent variable (eg. time), \\(y\\) is the dependent variable, and \\(\\sigma\\) is the associated measurement error in \\(y\\).\nAs new data arrives \\((x_0,y_0,\\sigma_0)\\) we shift the indices of prior data to make room, and scale up the errors by some factor \\(\\gamma\\in (0,1)\\):\n\\[ (x_{i+1},y_{i+1},\\sigma_{i+1})\\leftarrow (x_i, y_i,\\sigma_i/\\gamma). \\]If we define \\(\\sigma_i^*\\) as the original value of \\(\\sigma_0\\) then after applying \\(i\\) of the above operations\n\\[ \\sigma_i = \\sigma_i^*/\\gamma^i \\] (2.1) so, since \\(\\gamma\u003c1\\), the historical deviations grow exponentially as new information is acquired.\nWe wish to fit data to a model which is a linear combination of any \\(M\\) specified functions of \\(x\\). For example, the functions could be \\(1,x,x^2,\\ldots,x^{M-1}\\), in which case their general linear combination,\n\\[ y(x) = a_1 + a_2 x + \\cdots + a_M x^{M-1} \\]is a polynomial of degree \\(M-1\\). The general form of this kind of model is\n\\[ y(x) = \\sum_{j=1}^M a_j X_j(x) \\] (2.2) where \\(X_1(x),\\ldots,X_M(x)\\) are arbitrary fixed functions of \\(x\\), called the basis functions. Note that the functions \\(X_j(x)\\) can be wildly nonlinear functions of \\(x\\). In this discussion “linear” refers only to the model’s dependence on its parameters \\(a_j\\).\nFor these linear models we define a merit function\n\\[ \\chi^2 = \\sum_{i=0}^N \\left[ \\frac{y_i - \\sum_j a_j X_j(x_i)}{\\sigma_i} \\right]^2. \\] (2.3) We will pick as best parameters those that minimize \\(\\chi^2\\). There are several different techniques for finding this minimum. We will focus on one: the singular value decomposition of the normal equations. To introduce it we need some notation.\nLet \\({\\bf A}\\) be a matrix whose \\(N\\times M\\) components are constructed from the \\(M\\) basis functions evaluated at the \\(N\\) abscissas \\(x_i\\), and from the \\(N\\) measurement errors \\(\\sigma_i\\), by the prescription\n\\[ A_{ij} = \\frac{X_j(x_i)}{\\sigma_i}. \\] (2.4) The matrix \\({\\bf A}\\) is called the design matrix of the fitting problem. Notice that in general \\(\\bf A\\) has more rows than columns, \\(N\\geq M\\), since there must be more data points than model parameters to be solved for.\nAlso define a vector \\(\\bf b\\) of length \\(N\\) by\n\\[ b_i = \\frac{y_i}{\\sigma_i} \\]and denote the \\(M\\) vector whose components are the parameters to be fitted, \\(a_1,\\ldots,a_M\\), by \\(\\bf a\\).\nIf we take the derivative of Eq. (2.3) with respect to all \\(M\\) parameters \\(a_j\\), we obtain \\(M\\) equations that must hold at the chi-square minimum,\n\\[ 0 = \\frac{1}{\\sigma_i^2} \\left[ y_i - \\sum_j a_j X_j(x_i) \\right] X_k(x_i)\\;\\; k=1,\\ldots,M. \\] (2.5) Interchanging the order of summations, we can write Eq. (2.5) as the matrix equation\n\\[ \\sum_j \\alpha_{kj} a_j = \\beta_k \\] (2.6) where\n\\[ \\alpha_{kj} = \\sum_i \\frac{X_j(x_i) X_k(x_i)}{\\sigma_i^2} \\text{ or equivalently } [\\alpha] = {\\bf A}^T\\cdot {\\bf A} \\] (2.7) an \\(M\\times M\\) matrix, and\n\\[ \\beta_k = \\sum_i \\frac{y_i X_k(x_i)}{\\sigma_i^2} \\text{ or equivalently } [\\beta] = {\\bf A}^T\\cdot {\\bf b} \\] (2.8) a vector of length \\(M\\).\nEq. (2.5) or (2.6) are called the normal equations of the least-squares problem. They can be solved for the vector parameters \\(\\bf a\\) by singular value decomposition (SVD). SVD solves fixes many difficulties in the normal equations, including susceptibility to round-off errors. SVD can be significantly slower than other methods; however, its great advantage, that it (theoretically) cannot fail, more than makes up for the speed disadvantage. A good review of SVD techniques can be found in Press et al. (\rCitation: 1992 Press, \rW., \rTeukolsky, \rS., \rVetterling, \rW. \u0026 Flannery, \rB. \r(1992).\r \rNumerical Recipes in C: The Art of Scientific Computing (Second).\r \rCambridge University Press. Retrieved from \rhttp://www.nr.com/\r)\rSection 2.6. In matrix form, the normal equations can be written as\n\\[ [\\alpha]\\cdot {\\bf a}=[\\beta]. \\] (2.9) 3. Covariance matrix Let us define\n\\[ [C] = [\\alpha]^{-1}. \\] (3.1) Then\n\\[ {\\bf a}=[C]\\cdot [\\beta] \\text{ or } a_j = \\sum_k C_{jk} \\beta_k \\]which allows us to determine \\(\\bf a\\)’s dependence on the \\(y_i\\) values. From Eq. (2.7) we see that \\([C]\\) is independent of \\(y_i\\) so\n\\[ a_j = \\sum_j C_{jk} \\sum_i A_{ik} \\frac{y_i}{\\sigma_i} \\] (3.2) and\n\\[ \\frac{\\partial a_j}{\\partial y_i} = \\sum_k C_{jk} \\frac{A_{ik}}{\\sigma_i}. \\]The covariance between two parameters \\(a_j\\) and \\(a_k\\) is defined as\n\\[ \\begin{array}{rcl} \\text{Covar}[a_j,a_k] \u0026 \\equiv \u0026 \\sum_i \\sigma_i^2 \\frac{\\partial a_j}{\\partial y_i} \\frac{\\partial a_k}{\\partial y_i} \\\\ \u0026 = \u0026 \\sum_i \\sigma_i^2 \\sum_{lm} C_{jl} \\frac{A_{il}}{\\sigma_i} \\frac{A_{im}}{\\sigma_i} \\\\ \u0026 = \u0026 \\sum_{lm} C_{jl} C_{km} \\alpha_{ml} \\end{array} \\] (3.3) but since \\([C]=[\\alpha]^{-1}\\) so\n\\[ \\sum_m C_{km} \\alpha_{ml} = \\delta_{kl} \\text{ or } [C][\\alpha]={\\bf 1} \\]where \\(\\delta_{kl}\\) is the Kronecker delta function. Hence Eq. (3.3) reduces to\n\\[ \\text{Covar}[a_j,a_k] = C_{jk} \\] (3.4) and we find that \\([C]\\) is the covariance matrix. The variance of a single parameter \\(a_j\\) is simply defined as\n\\[ \\text{Var}[a_j]=\\text{Covar}[a_j,a_j]=C_{jj}. \\] (3.5) 4. Storage and updating So far we have made no mention of \\(N\\), the number of data points to be fit. As we will see an advantage of the discounted least squares method is that \\(N\\) becomes irrelevant. As data points are accumulated the oldest data becomes decreasingly relevant and eventually contribute negligibly to the fitting procedure. Hence we can theoretically apply this data to an infinite data set. But can this be practically implemented? The answer is…Yes!\nNotice that as we acquire new data \\((x_0,y_0,\\sigma_0)\\) according to Eq. (2.7) and (2.8) the matrix \\([\\alpha]\\) and vector \\([\\beta]\\) update as\n\\[ \\alpha_{kj} \\leftarrow A_{0j} A_{0k} + \\gamma^2 \\alpha_{kj} = \\frac{X_j(x_0) X_k(x_0)}{\\sigma_0^2} + \\gamma^2 \\alpha_{kj} \\] (4.1) and\n\\[ \\beta_j \\leftarrow A_{0j} b_0 + \\gamma^2 \\beta_j = \\frac{X_j(x_0) y_0}{\\sigma_0^2} + \\gamma^2 \\beta_j \\] (4.2) so it becomes clear that we need not even store the history of data points, but should rather store just \\([\\alpha]\\) and \\([\\beta]\\) and update them as new data is accumulated.\nA useful measure we have neglected to calculate so far is \\(\\chi^2\\),the chi-square statistic itself. In matrix notation Eq. (2.3) can be written\n\\[ \\begin{array}{rcl} \\chi^2 \u0026 = \u0026 ({\\bf a}^T \\cdot {\\bf A}^T - {\\bf b}^T) \\cdot ({\\bf A} \\cdot {\\bf a}-{\\bf b}) \\\\ \u0026 = \u0026 {\\bf a}^T \\cdot {\\bf A}^T \\cdot {\\bf A} \\cdot {\\bf a} - {\\bf b}^T \\cdot {\\bf A} \\cdot {\\bf a} - {\\bf a}^T \\cdot {\\bf A}^T \\cdot {\\bf b} + {\\bf b}^T \\cdot {\\bf b} \\\\ \u0026 = \u0026 {\\bf a}^T \\cdot ([\\alpha] \\cdot {\\bf a} - [\\beta]) - [\\beta]^T \\cdot {\\bf a} + {\\bf b}^T \\cdot {\\bf b} \\\\ \u0026 = \u0026 {\\bf b}^T \\cdot {\\bf b} - [\\beta]^T \\cdot {\\bf a} \\end{array} \\]which appears to still depend on the data history in the first term. Let us define this term as a new variable \\(\\delta\\),\n\\[ \\delta \\equiv {\\bf b}^T \\cdot {\\bf b} = \\sum_i b_i^2. \\]Then, similarly to Eq. (4.1) and (4.2) \\(\\delta\\) can be updated as more information is accumulated\n\\[ \\delta \\leftarrow b_0^2 + \\gamma^2 \\delta = \\frac{y_0^2}{\\sigma_0^2} + \\gamma^2 \\delta. \\] (4.3) ​\rFigure 4.1\rFigure 4.1: Discounted least-squares fitting has a computational storage advantage over traditional least-squares when \\(N\u003eM^2+M+4\\) where \\(N\\) is the number of data points and \\(M\\) is the number of parameters to be fitted.\nSo, to store all relevant history information we need only remember \\([\\alpha]\\), \\([\\beta]\\), and \\(\\delta\\) as well as the latest data triplet \\((x_0,y_0,\\sigma_0)\\) for a total of \\(M^2+M+4\\) numbers, regardless of how many data points have been acquired. Figure 4.1 shows that for many practical problems discounted least-squares fitting requires less storage than other methods. Although it has not been tested, we expect a similar condition to hold for processing time because the number of calculations depend only on \\(M\\) instead of \\(M\\) and \\(N\\) as in traditional least-squares methods.\nAs the reader can justify, all of these values should be initialized (prior to any data) with null values: \\([\\alpha]={\\bf 0}\\), \\([\\beta]={\\bf 0}\\), and \\(\\delta=0\\).\n5. Memory, effective number of data points For traditional least-squares fitting it is well known that if the measurement errors of \\(y_i\\) are distributed normally then the method is a maximum likelihood estimation and the expectation value (average) of Eq. (2.3) evaluates to\n\\[ \\left\\langle \\chi^2 \\right\\rangle = N-M. \\]This arises because \\((y_i - y(x_i))/\\sigma_i\\) is distributed normally with mean 0 and variance 1, so the sum of \\(N\\) variances should equate to \\(N\\). The subtraction of \\(M\\) is necessary because \\(M\\) parameters can be adjusted to actually reduce the variances further. For instance, with \\(N=M=1\\) we can adjust the single parameter such that the curve passes precisely through the point \\((x_0,y_0)\\), with zero variance. Similarly, for our method\n\\[ \\begin{array}{rcl} \\left\\langle \\chi^2 \\right\\rangle \u0026 = \u0026 \\sum_{i=0}^\\infty \\gamma^{2 i} \\left\\langle \\left[ \\frac{y_i-y(x_i)}{\\sigma_i^*}\\right]^2 \\right\\rangle - M \\\\ \u0026 = \u0026 \\sum_{i=0}^\\infty \\gamma^{2 i} - M \\\\ \u0026 = \u0026 \\frac{1}{1-\\gamma^2} - M \\end{array} \\] (5.1) which strongly suggests an effective number of data points\n\\[ N_{eff} \\equiv \\frac{1}{1-\\gamma^2}. \\] (5.2) 5.1. Memory We now undertake a thought experiment to understand how \\(N_{eff}\\) comes into play. Consider a single parameter fit \\(X(x_i)=1\\) or \\(y(x_i)=a\\) to a long history of values \\(y_i=1\\) (regardless of \\(x_i\\)). Updating according to Eq. (4.1), (4.2), and (4.3) gives\n\\[ \\begin{array}{rcr} \\alpha \u0026 \\leftarrow \u0026 1 + \\gamma^2 \\alpha \\\\ \\beta \u0026 \\leftarrow \u0026 y + \\gamma^2 \\beta \\\\ \\delta \u0026 \\leftarrow \u0026 y^2 + \\gamma^2 \\delta \\end{array} \\]so after a long history of \\(y=1\\),\n\\[ \\begin{array}{rl} \\alpha \u0026 = 1 + \\gamma^2 \\left( 1 + \\gamma^2(\\cdots)\\right) = 1 + \\gamma^2 + \\gamma^4 + \\cdots \\\\ \u0026 = \\frac{1}{1-\\gamma^2} = \\beta = \\delta \\end{array} \\]which has a solution, in one dimension, of\n\\[ a = \\frac{\\beta}{\\alpha} \\]or \\(y(x)=a=1\\).\nNow consider a sudden shift in the data stream to \\(y_i=0\\) (like a step function). How will this change our curve fit? The value of \\(\\alpha\\) remains unchanged but \\(\\beta\\) and \\(a\\) change as follows:\n\\(y\\) \\(\\beta\\) \\(a\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(1\\) \\(\\alpha\\) \\(1\\) \\(0\\) \\(\\gamma^2 \\alpha\\) \\(\\gamma^2\\) \\(0\\) \\(\\gamma^4 \\alpha\\) \\(\\gamma^4\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) Notice that \\(a\\) decays exponentially to the new equilibrium \\(a=0\\). The time constant \\(\\tau\\) for the system (time for \\(a\\) to decay to \\(1/e\\)) is\n\\[ \\begin{array}{l} \\left( \\gamma^2 \\right)^\\tau = e^{-1} \\\\ \\Rightarrow \\tau = \\frac{-1}{2 \\ln \\gamma} \\end{array} \\]which is, as Figure 5.1 shows, almost identical to \\(N_{eff}\\) for all \\(\\gamma\\).\n​\rFigure 5.1\rFigure 5.1: The difference between \\(N_{eff}\\) and \\(\\tau\\) is strictly less than one. The main difference between the two is that \\(\\sigma_{N_{eff}} \\rightarrow \\infty \\) as \\(N_{eff}\\rightarrow 1\\) (as it should) while \\(\\sigma_\\tau = \\sqrt{e} \\sigma_\\tau^*\\) for all \\(\\tau\\). They converge as \\(N_{eff}\\rightarrow \\infty\\).\nThus, \\(N_{eff}\\) is indeed a practical measure of the effective number of data points in a fit. The fit is dominated by the most recent data \\(i\\leq N_{eff}\\), and \\(N_{eff}\\) acts as the memory of the fitting procedure.\n6. Unknown measurement errors On occasion measurement uncertainties are unknown and least-squares fitting can be used to recover an estimate of these uncertainties. Be forewarned that this technique assumes normally distributed y data with identical variances. If this is not the case, the results become meaningless. It also precludes the use of a “goodness-of-fit” estimator (such as the incomplete gamma function, see Press et al. (\rCitation: 1992 Press, \rW., \rTeukolsky, \rS., \rVetterling, \rW. \u0026 Flannery, \rB. \r(1992).\r \rNumerical Recipes in C: The Art of Scientific Computing (Second).\r \rCambridge University Press. Retrieved from \rhttp://www.nr.com/\r)\rSection 6.2) because it assumes a good fit.\nWe begin by assuming \\(\\sigma_i^*=1\\) for all data points and proceeding with our calculations of \\(\\bf a\\) and \\(\\chi^2\\). If all (unknown) variances are equal \\(\\sigma^*=\\sigma_i^*\\) then Eq. (5.1) actually becomes\n\\[ \\left\\langle \\chi^2 \\right\\rangle = (N_{eff} - M) \\sigma^{*2} \\]so the actual data variance should be\n\\[ \\sigma^{*2} = \\frac{\\chi^2}{N_{eff}-M}. \\] (6.1) We can update our parameter error estimates by recognizing that, from Eq. (3.2) and Eq. (2.4), the covariance matrix is proportional to the variance in the data, so\n\\[ C_{jk} \\leftarrow \\frac{\\chi^2}{N_{eff}-M} C_{jk}. \\] (6.2) 7. Forecasting Forecasting via curve fitting is a dangerous proposition because it requires extrapolating into a region beyond the scope of the data, where different rules may apply, and hence, different parameter values. Nevertheless, it often used simply for its convenience. We assume the latest parameter estimations apply at the forecasted point \\(x\\) and simply use Eq. (2.2) to predict \\(y(x)\\).\nThe uncertainty in the prediction can be estimated from the covariance matrix. The definition of variance for any distribution is the expectation value of the squared difference from the mean:\n\\[ \\text{Var}[z] \\equiv \\left\\langle \\left( z - \\langle z \\rangle \\right)^2 \\right\\rangle \\]and the covariance between two variables is defined as\n\\[ \\text{Covar}[z_1,z_2] \\equiv \\left\\langle \\left( z_1 - \\langle z_1 \\rangle \\right) \\left( z_2 - \\langle z_2 \\rangle \\right) \\right\\rangle \\]so Eq. (2.2) has a variance\n\\[ \\begin{array}{rcl} \\text{Var}[y(x)] \u0026 = \u0026 \\text{Var}\\left[ \\sum_j a_j X_j(x) \\right] \\\\ \u0026 = \u0026 \\left\\langle \\left( \\sum_j a_j X_j(x) - \\sum_j \\langle a_j \\rangle X_j(x) \\right) ^2 \\right\\rangle \\\\ \u0026 = \u0026 \\sum_{jk} X_j(x) \\left\\langle \\left( a_j - \\langle a_j \\rangle \\right) \\left( a_k - \\langle a_k \\rangle \\right) \\right\\rangle X_k(x) \\\\ \u0026 = \u0026 \\sum_{jk} X_j(x) \\text{Covar}[a_j,a_k] X_k(x) \\\\ \u0026 = \u0026 X_j(x) C_{jk} X_k(x) \\end{array} \\]where \\([C]\\) is the covariance matrix discussed in Section 3 with possible updating, in the absence of measurement errors, according to Eq. (6.2).\nThe above gives the uncertainty in \\(y(x)\\), but in our derivation we have assumed the observed \\(y\\)-values were distributed normally around the curve where \\(y(x)\\) represents the mean of the distribution. Similarly for our prediction, \\(y(x)\\) is the prediction of the mean with its own uncertainty—-on top of which there is the measurement uncertainty of data around the mean. If the expected measurement uncertainty is given by \\(\\sigma'\\) then the predicted observation \\(y' \\sim N(y(x),\\sigma')\\) or, with the substitution\n\\[ z' = y' - y(x) \\]we assume \\(z' \\sim N(0,\\sigma')\\) regardless of the prediction \\(y(x)\\). In other words, \\(z'\\) and \\(y(x)\\) are mutually independent.\n\\[ \\begin{array}{rcl} \\text{Var}[y'] \u0026 = \u0026 \\text{Var}[y(x) + z'] \\\\ \u0026 = \u0026 \\left\\langle \\left( y(x) - \\langle y(x) \\rangle + z' - \\langle z' \\rangle \\right)^2 \\right\\rangle \\\\ \u0026 = \u0026 \\left\\langle \\left( y(x) - \\langle y(x) \\rangle \\right)^2 + \\left( z' - \\langle z' \\rangle \\right)^2 + 2 \\left( y(x) - \\langle y(x) \\rangle \\right) \\left( z' - \\langle z' \\rangle \\right) \\right\\rangle \\\\ \u0026 = \u0026 \\left\\langle \\left( y(x) - \\langle y(x) \\rangle \\right)^2 \\right\\rangle + \\left\\langle \\left( z' - \\langle z' \\rangle \\right)^2 \\right\\rangle + 2 \\left\\langle \\left( y(x) - \\langle y(x) \\rangle \\right) \\left( z' - \\langle z' \\rangle \\right) \\right\\rangle \\\\ \u0026 = \u0026 \\text{Var}[y(x)] + \\text{Var}[z']. \\end{array} \\]The last step of dropping the covariance term is allowed because when two distributions are independent\n\\[ \\left\\langle \\left( z_1 - \\langle z_1 \\rangle \\right) \\left( z_2 - \\langle z_2 \\rangle \\right) \\right\\rangle = \\left\\langle z_1 - \\langle z_1 \\rangle \\right\\rangle \\left\\langle z_2 - \\langle z_2 \\rangle \\right\\rangle = 0 \\]so our final results for the forecast \\(y'\\) at \\(x\\) are\n\\[ \\begin{array}{rcl} \\langle y' \\rangle \u0026 = \u0026 y(x) = \\sum_j a_j X_j(x) \\\\ \\text{Var}[y'] \u0026 = \u0026 \\sum_{jk} X_j(x) C_{jk} X_k(x) + \\sigma'^2. \\end{array} \\] (7.1) If \\(\\sigma'\\) is unknown it should be set to the same scale as historical measurement errors. If they are unknown, \\(\\sigma'\\) should be estimated from Eq. (6.1).\n8. Implementation A sample implementation of the above method, using a polynomial fitting function is included in the DOS program dpolyfit.exe((Download {{:random_research:rik_s_notes:discounted_least_squares:dls.zip|dls.zip}} source code and executable, released to the public domain)). It is designed to make continuously updated parameter fits from data in the input stream, and output these parameters, or forecasts derived therefrom, to the output stream. It takes as a command-line parameter an initialization file containing preset parameters for the input format, fitting procedure, and output display. Common usages of this program are:\n​\rListing 8.1\rdpolyfit.exe inifile.ini \u003cin.dat \u003eout.dat dpolyfit.exe inifile.ini \u003cin.dat \u003e\u003eout.dat\rListing 8.1: Sample command-line parameters for program, where inifile.ini, in.dat, and out.dat are replaced with desired configuration, input and output files, respectively. The second version appends output to out.dat rather than overwriting it.\nThe behaviour of the program is controlled by the initialization file which contains the following options:\n​\rListing 8.2\r[Input] Errors=No\t; input s values? [Fit] Memory=14\t; effective # data points, Neff in Eq. 5.2 Parameters=7\t; # of parameters a to fit, M in Eq. 2.2 [Output] Input=Yes\t; print input (x,y,s) triplet to output? Parameters=No\t; print parameters a and errors to output? Forecast=Yes\t; print forecasted y to output? Forecast Distance=0\t; forecast y at x=x0+\"Forecast Distance\" [Abort]\t; end program when all of the following are true: x=0 y=0 sig=0\t; only compare s if \"[Input] Errors=Yes\"\rListing 8.2: Sample initialization file inifile.ini.\nIf “[Input] Errors=Yes” then the program expects three numbers per data point \\((x_0,y_0,\\sigma_0)\\), each separated by white spaces (eg. “1.0 1.2 0.4”). Otherwise, only two are expected \\((x_0,y_0)\\).\nThe option “[Fit] Memory=...” contains \\(N_{eff}\\) as in Eq. (5.2). Generally, this must be strictly \\(\u003e0\\), but if a negative number is entered it is interpreted as \\(N_{eff}=\\infty\\), producing a traditional fit over all data points with no rescaling of the measurement errors.\nIf “[Output] Input=Yes” then the input values \\((x_0,y_0,\\sigma_0)\\) are reproduced as the first three columns of the output file. Even if \\(\\sigma_0\\) is unspecified, a best estimate is output, based on Eq. (6.1).\nIf “[Output] Parameters=Yes” then the next \\(2 M\\) columns of output are the parameters \\(\\bf a\\) and their respective uncertainties, eg. “\\(a_1 \\delta a_1 \\ldots a_M \\delta a_M\\)”.\nIf “[Output] Forecast=Yes” then the program generates another two columns of output. It calculates the forecast of \\(y\\) at \\(x=x_0+\\)\"[Output] Forecast Distance\" from Eq. (7.1) and prints out the forecasted \\(y\\) and its uncertainty.\nThe program ends when the input \\((x_0,y_0,\\sigma_0)\\) or \\((x_0,y_0)\\) matches the values in “[Abort] x=... y=... sig=...” or “[Abort] x=... y=...”, respectively.\nThe design of this program might cause some confusion: even though the input data file is a complete set of data, the program only analyzes each data point successively, and all output is based on just this point and prior data. For example, forecasts are based only on past data, so by the end of the output file, all data points are considered but in the beginning results are based only on a single data point. The technique derived herein is better suited to time series with slowly accumulating data, rather than a complete data set on startup.\n9. Summary As in traditional least-squares methods, by differentiating the \\(\\chi^2\\) merit function (Eq. (2.3)) of a linear model (Eq. (2.2)) we were able find a set of linear equations (Eq. (2.9)) which allowed us to solve for the optimal parameters which minimize \\(\\chi^2\\). By scaling up the error tolerance of old data as new data is acquired (Eq. (2.1)) we were able to compact the information such that only an \\(M \\times M\\) matrix \\([\\alpha]\\) (Eq. (4.1)), an \\(M\\times 1\\) vector \\([\\beta]\\) (Eq. (4.2)), and a scalar \\(\\delta\\) (Eq. (4.3)) need be stored to retain the full history of accumulated data. We found that the inverse of \\([\\alpha]\\) (Eq. (3.1)) held the covariance matrix of the fitted parameters (Eq. (3.4) and (3.5)). We were able to compute a memory for this technique (Eq. (5.2)) which is comparable to the number of data points used in traditional least-squares fitting. By assuming a good fit we were able to estimate the measurement uncertainties if they were unknown, and apply this to reconstruct reasonable deviations in the fitted parameters (Eq. (6.2)). Finally, by extrapolating with the latest parameter estimates (assuming they were valid in the forecasted domain), we were able to make forecasts and estimate their uncertainties (Eq. (7.1)).\nAs my list of references will attest, I have done virtually no research to see whether the discounted least-squares method has already been discovered (as is undoubtedly the case) and what name it goes by. In deriving this, I did not care if I “reinvented the wheel” because my goal was to introduce myself to some of the statistical techniques, particularly those used in the derivation of uncertainties.\n10. References Blok\r(1997)\rBlok, \rH.(1997, 7/20). Retrieved from \rhttp://www.zoology.ubc.ca/~rikblok/wiki/doku.php?id=random_research:rik_s_notes:discounted_least_squares:start\rPress, \rTeukolsky, \rVetterling \u0026 Flannery\r(1992)\rPress, \rW., \rTeukolsky, \rS., \rVetterling, \rW. \u0026 Flannery, \rB. \r(1992).\r \rNumerical Recipes in C: The Art of Scientific Computing (Second).\r \rCambridge University Press. Retrieved from \rhttp://www.nr.com/",
    "description": "Discounted Least Squares Curve Fitting",
    "tags": [],
    "title": "Blok (1997)",
    "uri": "/~rikblok/research/notes/discounted_least_squares/index.html"
  },
  {
    "breadcrumb": "Research ramblings \u003e Published",
    "content": "Effect of boundary conditions on scaling in the “game of Life” Blok \u0026 Bergersen\r(1997)\rBlok, \rH. \u0026 Bergersen, \rB.\r \r(1997).\r Effect of boundary conditions on scaling in the “game of Life”.\rPhysical Review E, 55(5). 6249–6252.\rhttps://doi.org/10.1103/PhysRevE.55.6249\rAbstract The debate as to whether the “game of Life” is self-organized critical remains unresolved. We present evidence that boundary conditions play an important role in the scaling behaviour, resulting in apparently contradictory results. We develop an analytic form for the scaling function and demonstrate that periodic boundaries force saturation, while open boundaries exhibit no such transitions on similar scales. We also consider the removal of boundaries altogether.\nDownload",
    "description": "Effect of boundary conditions on scaling in the “game of Life”",
    "tags": [],
    "title": "Blok \u0026 Bergersen (1997)",
    "uri": "/~rikblok/research/published/blok97/index.html"
  },
  {
    "breadcrumb": "Mathematical musings",
    "content": "Pythagorean theorem\rIn a right triangle the square of the hypotenuse is equal to the sum of the squares of the sides containing the right angle.\nI’ve been puzzling over a proof for this for years, and it finally dawned on me. (Eureka!) It’s all in how you draw it…\nProof #1 Given the triangle formed by $a$, $b$ (choosing $b\\geq a$) and $c$, we can construct a square with total area $c^2$. As shown, we can fit four triangles, each with area $a b/2$, into the large square, leaving an inner square with area $(b-a)^2$. Thus, the total area of the large square is\n$$ \\begin{array}{rl} c^2 \u0026 = 4 (a b/2) + (b-a)^2 \\\\ \u0026 = 2 a b + a^2 + b^2 - 2 a b \\\\ \u0026 = a^2 + b^2 . \\end{array} $$Hence, the Pythagorean theorem.\nProof #2 I found another proof, which Jim Loy told me is due to Legendre. It relies on recognizing that you can subdivide a triangle forming two sub-triangles similar to each other and the original. (I won’t prove this.) Then, from the figure above, and from the properties of similar triangles\n$$ \\frac{a}{e} = \\frac{c}{a} \\text{ thus } a^2 = c e $$and\n$$ \\frac{b}{f} = \\frac{c}{b} \\text{ thus } b^2 = c f. $$Adding the two results together gives\n$$ \\begin{array}{rl} a^2 + b^2 \u0026 = c e + c f \\\\ \u0026 = c (e+f) \\\\ \u0026 = c^2 . \\end{array} $$Hence, the Pythagorean theorem.\n— Rik Blok, 1997",
    "description": "I’ve been puzzling over a proof for this for years, and it finally dawned on me. (Eureka!) It’s all in how you draw it…",
    "tags": [],
    "title": "Proof of the Pythagoran theorem (1997)",
    "uri": "/~rikblok/math/pythagoras/index.html"
  },
  {
    "breadcrumb": "Research ramblings",
    "content": "My peer-reviewed and published articles.\nDoebeli, Blok, Leimar \u0026 Dieckmann (2007)Multimodal pattern formation in phenotype distributions of sexual populations\nPineda-Krch, Blok, Dieckmann \u0026 Doebeli (2007)A tale of two cycles - distinguishing quasi-cycles and limit cycles in finite predator-prey populations\nKillingback, Blok \u0026 Doebeli (2006)Scale-free extinction dynamics in spatially structured host–parasitoid systems\nBlok (2000)On the nature of the stock market: Simulations and experiments\nBlok \u0026 Bergersen (1999)Synchronous versus asynchronous updating in the “game of Life”\nBlok \u0026 Bergersen (1997)Effect of boundary conditions on scaling in the “game of Life”",
    "description": "My peer-reviewed and published articles",
    "tags": [],
    "title": "Published",
    "uri": "/~rikblok/research/published/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Contact me below if you’d like to talk to me outside of class. Also, here’s some course material for current and past courses I have been involved in.\nContact RikWant to book an appointment or reach me? Check out my office hours and contact information here.\nWorkloadView my historical workload, automatically generated from the emails waiting in my inbox.\nRecent coursesHere are some recent courses I’ve taught.\nUBC CPSC 110Computation, Programs, and Programming\nFundamental program and computation structures. Introductory programming skills. Computation as a tool for information processing, simulation and modelling, and interacting with the world.\nUBC CPSC 107Systematic Program Design\nFundamental computation and program structures. Continuing systematic program design from CPSC 103.\nUBC ISCI 320Research Development Project\nRetreat to develop skills in writing scientific research proposals. Emphasis on formulating and testing hypotheses to explain observations.\nOld coursesHere are some courses I’ve taught in the past.\nUBC ISCI 344 (2016)Game Theory in Economics and Evolution (Fall 2016))\nExploration of human and animal interactions: integrating evolutionary and economic perspectives to investigate individual and social behaviour.\nUBC ISCI 422 (2009)Models in Science (Fall 2009)\nMeaning, nature, use, strengths and limitations of models as investigative tools in all scientific disciplines. Detailed investigation of selected model systems from different scientific disciplines.\nUBC PHYS 102 (2003)Electricity, Light and Radiation (Summer 2003)\nIntroduction to optics, electricity and magnetism, electric circuits, radioactivity, including biological applications.\nUBC PHYS 153 (2000)Elements of Physics (Winter 2000)\nThermometry, thermal properties of matter, heat, oscillations, waves, sound, wave optics; geometrical optics, elementary electricity and magnetism, simple DC and AC circuits.",
    "description": "My contact information and material from some of my courses.  Look here if you want to book an appointment.",
    "tags": [],
    "title": "Academic anecdotes",
    "uri": "/~rikblok/teaching/index.html"
  },
  {
    "breadcrumb": "Academic anecdotes",
    "content": "Want to book an appointment or reach me? Check out my office hours and contact information here.\nOffice hours Choose an available time slot to book an online Zoom appointment:\nE-mail rik.blok@ubc.ca Go ahead, fire off an email if you have a question or want to meet outside of my regular office hours (above). It may take me a while to get back to you, depending on my workload.\nPhone 604-736-6343 Feel free to phone me at home anytime. Leave a message if I’m not around.\nSnail mail Rik Blok, Sessional Lecturer Computer Science, UBC 201 - 2366 Main Mall Vancouver, BC, Canada V6T 1Z4 I don’t check my mailbox regularly so please let me know if you’ve mailed me something.",
    "description": "Want to book an appointment or reach me?  Check out my office hours and contact information here.",
    "tags": [],
    "title": "Contact Rik",
    "uri": "/~rikblok/teaching/contact/index.html"
  },
  {
    "breadcrumb": "Research ramblings",
    "content": "Work I’ve presented for feedback at seminars, lab meetings, and guest lectures.\nBlok (2003)Self-affine timeseries analysis. Guest lecture for U.B.C. Physics 510: Stochastic Processes in Physics\nBlok (2002)Rock, paper and scissors in space: A demonstration of R2DToo.\nBlok (2002)Statistical properties of financial timeseries\nBlok (2001)Can memes drive genes?\nBlok (2000)On the nature of the stock market: Simulations and experiments. Final PhD oral defense\nBlok (2000)On the nature of the stock market: Simulations and experiments. Departmental PhD oral defense\nBlok (1998)Modelling intentionality: The gambler. Presentation for UBC Phys 510\nBlok (1998)Extra! Extra! Critical update on ‘Life’. Presentation for Peter Wall Inst. Adv. Science, Crisis Points Group, UBC",
    "description": "Work I’ve presented for feedback at seminars, lab meetings, and guest lectures",
    "tags": [],
    "title": "Presented",
    "uri": "/~rikblok/research/presented/index.html"
  },
  {
    "breadcrumb": "",
    "content": "I spend a lot of time around computers, both at work and play. Yes, I’m a card-carrying geek 😉 Here’s what I’ve picked up over the years.",
    "description": "I spend a lot of time around computers, both at work and play. Yes, I’m a card-carrying geek 😉  Here’s what I’ve picked up over the years.",
    "tags": [],
    "title": "Computational capers",
    "uri": "/~rikblok/compute/index.html"
  },
  {
    "breadcrumb": "Academic anecdotes",
    "content": "You reached out to me and now you’re wondering, “That ahle! Why hasn’t Rik got back to me?” If I’m busy I just may not have had a chance yet. So check out my current workload for yourself. Automatically generated from the number of emails waiting in my inbox.\n​\r1 week\r2 months\r1 year\r10 years\rall time\rChart shows daily maximum workload.\nChart shows daily maximum workload.\nChart shows weekly maximum workload.\nChart shows 3-month maximum workload.\nChart shows 3-month maximum workload.\nUnder light workloads expect a response within a few days. I should be able to get back to you within a week under moderate workloads. But when my workload is heavy it could take a few weeks. Also check my schedule: I may be in a meeting or out of town.",
    "description": "View my historical workload, automatically generated from the emails waiting in my inbox.",
    "tags": [],
    "title": "Workload",
    "uri": "/~rikblok/teaching/workload/index.html"
  },
  {
    "breadcrumb": "Research ramblings",
    "content": "Here are some research-oriented technical notes I’ve written. They’re not peer-reviewed or published.\nBlok (1997)Discounted Least Squares Curve Fitting",
    "description": "Here are some research-oriented technical notes I’ve written. They’re not peer-reviewed or published.",
    "tags": [],
    "title": "Unpublished notes",
    "uri": "/~rikblok/research/notes/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Some interesting (to me!) mathematical puzzles and problems I’ve come across.\nEuler's constant (2014)I found a sequence that converges to Euler’s constant faster than Bernoulli’s formula.\nShared gas (2002)My wife and I live in a twenty eight unit condominium which shares the cost of natural gas. At what price level can we expect the residents to switch from heating with gas to heating with electricity?\nThe Pythagorean theorem (1997)I’ve been puzzling over a proof for this for years, and it finally dawned on me. (Eureka!) It’s all in how you draw it…",
    "description": "Some interesting (to me!) mathematical puzzles and problems I’ve come across.",
    "tags": [],
    "title": "Mathematical musings",
    "uri": "/~rikblok/math/index.html"
  },
  {
    "breadcrumb": "Academic anecdotes",
    "content": "Here are some recent courses I’ve taught.\nUBC CPSC 110Computation, Programs, and Programming\nFundamental program and computation structures. Introductory programming skills. Computation as a tool for information processing, simulation and modelling, and interacting with the world.\nUBC CPSC 107Systematic Program Design\nFundamental computation and program structures. Continuing systematic program design from CPSC 103.\nUBC ISCI 320Research Development Project\nRetreat to develop skills in writing scientific research proposals. Emphasis on formulating and testing hypotheses to explain observations.",
    "description": "Here are some recent courses I’ve taught.",
    "tags": [],
    "title": "Recent courses",
    "uri": "/~rikblok/teaching/current/index.html"
  },
  {
    "breadcrumb": "",
    "content": "I’m a theoretical statistical physicist by training and a complexologist by nature. The common thread throughout my research is the search for common features of complex, irreducible systems.\nTraditionally, science approaches a problem by breaking it into parts and solving each part separately. In some cases, even when the individual pieces are well understood, the interactions between them will lead to surprising outcomes. Many important and diverse systems exhibit this irreducibility: earthquakes, ecosystems, stock markets, weather, computer networks, the immune system, the brain, forest fires, et cetera. Traditional scientific methods are ill-equipped to cope with these complex systems so my approach is to use novel tools such as nonequilibrium statistical physics theory and computer simulations to further our understanding.\nSee also my Google Scholar profile or my Zotero library.\nPublishedMy peer-reviewed and published articles\nDoebeli, Blok, Leimar \u0026 Dieckmann (2007)Multimodal pattern formation in phenotype distributions of sexual populations\nPineda-Krch, Blok, Dieckmann \u0026 Doebeli (2007)A tale of two cycles - distinguishing quasi-cycles and limit cycles in finite predator-prey populations\nKillingback, Blok \u0026 Doebeli (2006)Scale-free extinction dynamics in spatially structured host–parasitoid systems\nBlok (2000)On the nature of the stock market: Simulations and experiments\nBlok \u0026 Bergersen (1999)Synchronous versus asynchronous updating in the “game of Life”\nBlok \u0026 Bergersen (1997)Effect of boundary conditions on scaling in the “game of Life”\nPresentedWork I’ve presented for feedback at seminars, lab meetings, and guest lectures\nBlok (2003)Self-affine timeseries analysis. Guest lecture for U.B.C. Physics 510: Stochastic Processes in Physics\nBlok (2002)Rock, paper and scissors in space: A demonstration of R2DToo.\nBlok (2002)Statistical properties of financial timeseries\nBlok (2001)Can memes drive genes?\nBlok (2000)On the nature of the stock market: Simulations and experiments. Final PhD oral defense\nBlok (2000)On the nature of the stock market: Simulations and experiments. Departmental PhD oral defense\nBlok (1998)Modelling intentionality: The gambler. Presentation for UBC Phys 510\nBlok (1998)Extra! Extra! Critical update on ‘Life’. Presentation for Peter Wall Inst. Adv. Science, Crisis Points Group, UBC\nUnpublished notesHere are some research-oriented technical notes I’ve written. They’re not peer-reviewed or published.\nBlok (1997)Discounted Least Squares Curve Fitting",
    "description": "I’m a theoretical statistical physicist by training and a complexologist by nature. The common thread throughout my research is the search for common features of complex, irreducible systems.",
    "tags": [],
    "title": "Research ramblings",
    "uri": "/~rikblok/research/index.html"
  },
  {
    "breadcrumb": "Academic anecdotes",
    "content": "Here are some courses I’ve taught in the past. These pages are not maintained and may contain broken links.\nUBC ISCI 344 (2016)Game Theory in Economics and Evolution (Fall 2016))\nExploration of human and animal interactions: integrating evolutionary and economic perspectives to investigate individual and social behaviour.\nUBC ISCI 422 (2009)Models in Science (Fall 2009)\nMeaning, nature, use, strengths and limitations of models as investigative tools in all scientific disciplines. Detailed investigation of selected model systems from different scientific disciplines.\nUBC PHYS 102 (2003)Electricity, Light and Radiation (Summer 2003)\nIntroduction to optics, electricity and magnetism, electric circuits, radioactivity, including biological applications.\nUBC PHYS 153 (2000)Elements of Physics (Winter 2000)\nThermometry, thermal properties of matter, heat, oscillations, waves, sound, wave optics; geometrical optics, elementary electricity and magnetism, simple DC and AC circuits.",
    "description": "Here are some courses I’ve taught in the past.",
    "tags": [],
    "title": "Old courses",
    "uri": "/~rikblok/teaching/past/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Science is both my work and my play. That doesn’t mean I’m particularly bright or hard-working, just that I’m curious. Because that’s the main ingredient for doing good science. (A healthy dose of skepticism helps, too.)",
    "description": "Science is both my work and my play. That doesn’t mean I’m particularly bright or hard-working, just that I’m curious. Because that’s the main ingredient for doing good science. (A healthy dose of skepticism helps, too.)",
    "tags": [],
    "title": "Scientific scribblings",
    "uri": "/~rikblok/science/index.html"
  },
  {
    "breadcrumb": "Academic anecdotes \u003e Recent courses",
    "content": "",
    "description": "Computation, Programs, and Programming\nFundamental program and computation structures. Introductory programming skills. Computation as a tool for information processing, simulation and modelling, and interacting with the world.",
    "tags": [],
    "title": "UBC CPSC 110",
    "uri": "/~rikblok/teaching/current/cpsc110/index.html"
  },
  {
    "breadcrumb": "Academic anecdotes \u003e Recent courses",
    "content": "",
    "description": "Systematic Program Design\nFundamental computation and program structures. Continuing systematic program design from CPSC 103.",
    "tags": [],
    "title": "UBC CPSC 107",
    "uri": "/~rikblok/teaching/current/cpsc107/index.html"
  },
  {
    "breadcrumb": "Academic anecdotes \u003e Recent courses",
    "content": "",
    "description": "Research Development Project\nRetreat to develop skills in writing scientific research proposals. Emphasis on formulating and testing hypotheses to explain observations.",
    "tags": [],
    "title": "UBC ISCI 320",
    "uri": "/~rikblok/teaching/current/isci320/index.html"
  },
  {
    "breadcrumb": "",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/~rikblok/categories/index.html"
  },
  {
    "breadcrumb": "",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/~rikblok/tags/index.html"
  }
]
